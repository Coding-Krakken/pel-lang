# ============================================================================
# PEL CODEBASE STATE SNAPSHOT
# ============================================================================
# Purpose: Neutral extraction of current implementation reality (as-is state)
# Date: 2026-02-13
# Version: 0.1.0
# Status: Pre-conversion baseline
# ============================================================================

metadata:
  snapshot_date: "2026-02-13T00:00:00Z"
  pel_version: "0.1.0"
  repository: "https://github.com/Coding-Krakken/pel-lang"
  total_loc_implementation: 6000
  total_loc_specification: 20000
  specification_documents: 11
  implementation_completeness: 0.65  # 65% - core compiler done, stdlib incomplete

# ============================================================================
# ONTOLOGY - System Components and Structure
# ============================================================================

ontology:
  architecture_type: "compiler_pipeline_with_runtime"
  primary_components:
    - compiler
    - runtime
    - stdlib
    - ir_schema
    - specifications
  
  component_hierarchy:
    compiler:
      type: "multi_stage_pipeline"
      stages:
        - lexer
        - parser
        - type_checker
        - provenance_checker
        - ir_generator
      entry_point: "compiler/compiler.py"
      total_loc: 2800
      
    runtime:
      type: "execution_engine"
      modes:
        - deterministic
        - monte_carlo
      entry_point: "runtime/runtime.py"
      total_loc: 400
      
    stdlib:
      type: "standard_library"
      modules:
        unit_econ: {status: complete, loc: 280, functions: 25}
        demand: {status: incomplete, loc: 0, functions: 0}
        funnel: {status: incomplete, loc: 0, functions: 0}
        pricing: {status: incomplete, loc: 0, functions: 0}
        cashflow: {status: incomplete, loc: 0, functions: 0}
        retention: {status: incomplete, loc: 0, functions: 0}
        capacity: {status: incomplete, loc: 0, functions: 0}
        hiring: {status: incomplete, loc: 0, functions: 0}
        shocks: {status: incomplete, loc: 0, functions: 0}
      completion_rate: 0.11  # 1/9 modules
      
    ir_schema:
      type: "canonical_representation"
      format: "json"
      schema_version: "0.1.0"
      location: "ir/pel_ir_schema.json"
      validation_rules: 15  # V001-V015
      
    specifications:
      type: "formal_documentation"
      count: 11
      total_lines: 9000
      coverage:
        language_syntax: complete
        formal_semantics: complete
        type_system: complete
        uncertainty: complete
        constraints: complete
        policies: complete
        governance: complete
        security: complete
        calibration: complete
        benchmarks: complete
        conformance: complete

# ============================================================================
# STATE VARIABLES - Compilation and Runtime State
# ============================================================================

state_variables:
  
  compilation_state:
    lexer_state:
      position: "Current character index in source"
      line: "Current line number (1-indexed)"
      column: "Current column number (1-indexed)"
      tokens: "List[Token] - accumulated token stream"
      type: "mutable during lexing, frozen after"
      
    parser_state:
      token_stream: "List[Token] - input from lexer"
      current_position: "Index into token stream"
      ast: "Model - root AST node being constructed"
      type: "mutable during parsing, frozen after"
      
    type_checker_state:
      environment: "Dict[str, Type] - identifier to type mapping (Γ)"
      ast: "Model - input AST"
      typed_ast: "Model - AST with type annotations"
      dimension_tracker: "Tracks dimensional analysis state"
      errors: "List[TypeError] - accumulated type errors"
      type: "produces new typed AST, input immutable"
      
    provenance_checker_state:
      ast: "Model - input typed AST"
      completeness_score: "float - [0.0, 1.0]"
      errors: "List[ProvenanceError]"
      warnings: "List[ProvenanceWarning]"
      type: "validation only, does not modify AST"
      
    ir_generator_state:
      ast: "Model - input validated AST"
      ir_document: "Dict - PEL-IR JSON structure"
      model_hash: "str - SHA-256 of canonical IR"
      assumption_hash: "str - SHA-256 of provenance metadata"
      dependency_graph: "Dict[str, List[str]] - topologically sorted"
      type: "produces new IR, input immutable"
  
  runtime_state:
    configuration:
      mode: "str - 'deterministic' | 'monte_carlo'"
      seed: "int - PRNG seed for reproducibility"
      num_runs: "int - Monte Carlo iterations (1 for deterministic)"
      time_horizon: "int - Maximum timestep T"
      
    execution_state:
      current_timestep: "int - t ∈ [0, T]"
      current_run: "int - Monte Carlo run index"
      variable_bindings: "Dict[str, Any] - current values"
      timeseries_results: "Dict[str, List[Any]] - historical values"
      constraint_violations: "List[Dict] - logged violations"
      policy_executions: "List[Dict] - logged policy triggers"
      prng_state: "numpy.random.RandomState - seeded generator"
      
    result_state:
      status: "str - 'success' | 'failed' | 'timeout'"
      timesteps_completed: "int"
      final_values: "Dict[str, Any]"
      statistics: "Dict - P10/P50/P90 for Monte Carlo"
      
  provenance_metadata:
    per_parameter:
      source: "str - REQUIRED - data origin"
      method: "str - REQUIRED - observed|fitted|derived|expert_estimate|external_research|assumption"
      confidence: "float - REQUIRED - [0.0, 1.0]"
      freshness: "str - RECOMMENDED - ISO 8601 duration"
      owner: "str - RECOMMENDED - responsible party"
      correlated_with: "List[Tuple[str, float]] - OPTIONAL - correlations"
      notes: "str - OPTIONAL - additional context"

# ============================================================================
# TRANSITIONS - State Machine Flows
# ============================================================================

transitions:
  
  compilation_pipeline:
    initial_state: "source_code_string"
    
    transitions:
      - from: "source_code"
        to: "token_stream"
        via: "Lexer.tokenize()"
        transforms: "Text → List[Token]"
        errors_possible: ["LexicalError (E00xx)"]
        invariants_established: ["All tokens valid", "Source locations tracked"]
        
      - from: "token_stream"
        to: "ast"
        via: "Parser.parse()"
        transforms: "List[Token] → Model (AST)"
        errors_possible: ["ParseError (E02xx)"]
        invariants_established: ["Syntactically valid AST", "All nodes well-formed"]
        
      - from: "ast"
        to: "typed_ast"
        via: "TypeChecker.check()"
        transforms: "Model → Model (with type annotations)"
        errors_possible: ["TypeError (E03xx)"]
        invariants_established: 
          - "Every expression has type"
          - "Dimensional correctness"
          - "Currency compatibility"
          - "No unit mismatches"
          
      - from: "typed_ast"
        to: "validated_ast"
        via: "ProvenanceChecker.check()"
        transforms: "Model → Model (no structural change)"
        errors_possible: ["ProvenanceError (E04xx)"]
        invariants_established:
          - "All params have provenance"
          - "Completeness score calculated"
          - "Methods validated"
          
      - from: "validated_ast"
        to: "ir_json"
        via: "IRGenerator.generate()"
        transforms: "Model → Dict (PEL-IR)"
        errors_possible: ["Internal compiler error"]
        invariants_established:
          - "Valid JSON structure"
          - "Conforms to pel_ir_schema.json"
          - "Model hash computed"
          - "Assumption hash computed"
          - "Dependencies topologically sorted"
    
    final_state: "ir_json"
    reversible: false
    deterministic: true
    
  runtime_execution:
    initial_state: "ir_json + RuntimeConfig"
    
    deterministic_mode:
      - state: "initialize"
        action: "Sample all distributions at mean/median"
        next: "timestep_loop"
        
      - state: "timestep_loop"
        action: "For t = 0 to T"
        substates:
          - "evaluate_variables (dependency order)"
          - "check_constraints"
          - "execute_policies"
          - "record_results"
        transitions:
          - condition: "fatal constraint violated"
            action: "Stop execution"
            next: "return_partial_results"
          - condition: "t < T"
            action: "Continue"
            next: "timestep_loop (t+1)"
          - condition: "t == T"
            action: "Finalize"
            next: "return_results"
            
    monte_carlo_mode:
      - state: "initialize"
        action: "Set PRNG seed"
        next: "run_loop"
        
      - state: "run_loop"
        action: "For run = 1 to N"
        substates:
          - "sample_distributions (full)"
          - "execute_deterministic_simulation"
          - "collect_run_results"
        next: "aggregate_statistics"
        
      - state: "aggregate_statistics"
        action: "Compute P10, P50, P90, success rate"
        next: "return_aggregated_results"
    
    final_states: ["success", "failed", "timeout"]
    
  policy_execution:
    trigger_evaluation:
      - "Evaluate trigger condition in current state"
      - "If true → execute action"
      - "If false → skip"
    action_types:
      set: "Assign new value to variable"
      multiply: "Scale variable by factor"
      add: "Increment variable"
    determinism: "Same state + same policies → same actions"
    order: "Declaration order preserved"

# ============================================================================
# INVARIANTS - Currently Enforced Properties
# ============================================================================

invariants:
  
  compile_time:
    type_safety:
      - "All expressions have valid types"
      - "No Currency<USD> + Currency<EUR>"
      - "No Rate + Duration (must multiply)"
      - "Dimensional analysis correct"
      
    causality:
      - "TimeSeries[t] only depends on TimeSeries[≤t]"
      - "No future references"
      
    provenance:
      - "All params have source, method, confidence"
      - "Confidence ∈ [0.0, 1.0]"
      - "Method ∈ {observed, fitted, derived, expert_estimate, external_research, assumption}"
      
    structural:
      - "Dependency graph acyclic (V001)"
      - "All dependencies exist (V002)"
      - "Distribution parameters valid (V007)"
      - "Correlation matrix positive semi-definite (V004)"
      
  runtime:
    reproducibility:
      - "Same seed → identical PRNG sequence"
      - "Same IR + seed → bit-identical results"
      
    numeric_safety:
      - "No divide-by-zero (guarded)"
      - "No out-of-bounds array access"
      
    constraint_semantics:
      - "Fatal violation → immediate stop"
      - "Warning violation → logged, continue"
      
    temporal:
      - "Timesteps monotonically increasing"
      - "t ∈ [0, T]"
      
  security:
    sandbox:
      - "No file I/O without capability"
      - "No network access without capability"
      - "No eval/exec/import allowed"
      - "Resource limits enforced"

# ============================================================================
# IO CONTRACTS - External Interfaces
# ============================================================================

io_contracts:
  
  compiler_interface:
    input:
      format: "UTF-8 text file"
      extension: ".pel"
      encoding: "utf-8"
      max_size: "unlimited (resource-limited)"
      
    output_success:
      format: "JSON"
      schema: "ir/pel_ir_schema.json"
      extension: ".ir.json"
      includes:
        - "Model structure (nodes, constraints, policies)"
        - "Metadata (hashes, timestamp, version)"
        - "Provenance data"
        
    output_error:
      format: "Structured error message"
      includes:
        - "Error code (E0xxx)"
        - "Error message"
        - "Source location (file, line, column)"
        - "Hint/suggestion"
      exit_code: 1
      
  runtime_interface:
    input:
      format: "PEL-IR JSON"
      schema: "ir/pel_ir_schema.json"
      config: "RuntimeConfig object"
      
    output_success:
      format: "JSON"
      structure:
        status: "success"
        mode: "deterministic | monte_carlo"
        seed: "int"
        timesteps: "int"
        variables: "Dict[str, List[Any]]"
        constraint_violations: "List[Dict]"
        policy_executions: "List[Dict]"
        statistics: "Dict (Monte Carlo only)"
      exit_code: 0
      
    output_failure:
      format: "JSON"
      structure:
        status: "failed | timeout"
        reason: "str"
        timesteps_completed: "int"
        partial_results: "Dict"
      exit_code: 0  # Graceful failure
      
  cli_interface:
    command: "pel <source.pel>"
    options:
      --mode: "deterministic | monte_carlo"
      --seed: "int (default: 42)"
      --runs: "int (default: 1000 for monte_carlo)"
      --output: "str (output file path)"
      --verbose: "bool (detailed logging)"

# ============================================================================
# DATA SCHEMAS
# ============================================================================

data_schemas:
  
  ast_schema:
    root_node: "Model"
    node_types:
      - "Model (name, time_horizon, time_unit, params, vars, funcs, constraints, policies)"
      - "ParamDecl (name, type_annotation, value, provenance)"
      - "VarDecl (name, type_annotation, value, is_mutable)"
      - "FuncDecl (name, parameters, return_type, body)"
      - "Constraint (name, condition, severity, message)"
      - "Policy (name, trigger, action)"
    expression_types:
      - "Literal, Variable, BinaryOp, UnaryOp, FunctionCall"
      - "Indexing, ArrayLiteral, Lambda, MemberAccess"
      - "IfThenElse, Distribution"
    type_annotations:
      - "Currency<ISO>, Rate per TimeUnit, Duration<TimeUnit>"
      - "Count<Entity>, Capacity<Resource>, Fraction"
      - "TimeSeries<T>, Distribution<T>, Scoped<T,E>"
      
  ir_schema:
    version: "0.1.0"
    schema_file: "ir/pel_ir_schema.json"
    validation: "JSON Schema v7"
    top_level:
      version: "str"
      model: "Object"
      metadata: "Object"
    model_structure:
      name: "str"
      time_horizon: "int"
      time_unit: "str"
      nodes: "List[Node]"
      constraints: "List[Constraint]"
      policies: "List[Policy]"
    node_types:
      - "param (parameters with provenance)"
      - "var (computed variables)"
      - "func (function declarations)"
    metadata_fields:
      model_hash: "SHA-256 hex string"
      assumption_hash: "SHA-256 hex string"
      compiled_at: "ISO 8601 timestamp"
      compiler_version: "str (e.g., pel-0.1.0)"
      assumption_completeness: "float [0.0, 1.0]"
      
  provenance_schema:
    required_fields:
      - "source: str"
      - "method: str"
      - "confidence: float"
    recommended_fields:
      - "freshness: str (ISO 8601 duration)"
      - "owner: str"
    optional_fields:
      - "correlated_with: List[Tuple[str, float]]"
      - "notes: str"
    validation:
      confidence_range: "[0.0, 1.0]"
      method_values: ["observed", "fitted", "derived", "expert_estimate", "external_research", "assumption"]

# ============================================================================
# SECURITY ENFORCEMENT POINTS
# ============================================================================

security_enforcement:
  
  compilation_security:
    input_validation:
      - "Source encoding validation (UTF-8)"
      - "Token validation (no binary data)"
      - "AST validation (no import statements)"
      
    resource_limits:
      - "Parser recursion depth (Python default)"
      - "AST size (unlimited, OS-limited)"
      
  runtime_security:
    sandbox_restrictions:
      disabled_builtins:
        - "open, file, input, raw_input"
        - "exec, eval, compile, execfile"
        - "__import__, reload"
        - "vars, dir, globals, locals (restricted)"
        
      ast_validation:
        - "No Import nodes"
        - "No file operations"
        - "No subprocess calls"
        
    resource_limits:
      memory: "2GB (configurable)"
      execution_timeout: "60s (configurable)"
      iteration_limit: "1M (compile-time check)"
      
    capability_system:
      default: "No I/O allowed"
      opt_in:
        file_read: "List of allowed paths"
        http: "List of allowed domains"
        
  data_security:
    input_sanitization:
      csv_injection: "Strip leading =, +, -, @"
      type_validation: "Runtime type checks"
      
    output_sanitization:
      no_secrets: "Never log sensitive data"
      structured_errors: "No stack traces to users"

# ============================================================================
# FAILURE HANDLING
# ============================================================================

failure_handling:
  
  compilation_failures:
    lexical_errors:
      codes: "E00xx"
      handling: "Stop immediately, report error"
      recovery: "None (must fix source)"
      
    syntax_errors:
      codes: "E02xx"
      handling: "Stop at first error, report location"
      recovery: "None (must fix source)"
      
    type_errors:
      codes: "E03xx"
      handling: "Stop at first error, provide hint"
      recovery: "None (must fix source)"
      
    provenance_errors:
      codes: "E04xx"
      handling: "Report all errors, stop compilation"
      recovery: "Add missing provenance metadata"
      
  runtime_failures:
    constraint_violations:
      fatal:
        handling: "Stop simulation immediately"
        output: "Partial results up to violation"
        logged: "Timestep, constraint name, values"
        
      warning:
        handling: "Log warning, continue execution"
        output: "Full results with warnings"
        logged: "All violations with context"
        
    timeout:
      handling: "Raise TimeoutError after N seconds"
      output: "Partial results if available"
      
    memory_limit:
      handling: "Raise MemoryError"
      output: "Error message only"
      
    numeric_errors:
      division_by_zero: "Guarded with conditional"
      overflow: "Python handles gracefully"
      nan_propagation: "Dependent on numpy settings"
      
  error_reporting:
    format: "Structured with code, message, location, hint"
    verbosity: "Configurable (normal | verbose)"
    exit_codes:
      success: 0
      compilation_error: 1
      validation_error: 1
      internal_error: 2

# ============================================================================
# OBSERVABILITY
# ============================================================================

observability:
  
  logging:
    current_state: "Print statements (ad-hoc)"
    levels: "Not structured (no DEBUG/INFO/WARN/ERROR)"
    destinations: "stdout/stderr only"
    
  metrics:
    current_state: "None implemented"
    planned:
      - "Compilation time"
      - "AST node count"
      - "Type checking time"
      - "Runtime execution time"
      - "Memory usage"
      
  tracing:
    current_state: "None implemented"
    
  debugging:
    compiler:
      - "Source location tracking (line, column)"
      - "Error messages with context"
      - "No interactive debugger"
      
    runtime:
      - "Timestep logging (verbose mode)"
      - "Variable value inspection"
      - "Constraint violation details"
      - "Policy execution log"

# ============================================================================
# PERFORMANCE ASSUMPTIONS
# ============================================================================

performance_assumptions:
  
  compilation:
    complexity:
      lexer: "O(n) - single pass"
      parser: "O(n) - recursive descent"
      type_checker: "O(n) - single pass with environment"
      provenance_checker: "O(p) - p = number of params"
      ir_generator: "O(n) - AST traversal"
    bottlenecks: "Type checking (complex expressions)"
    
  runtime:
    deterministic:
      complexity: "O(T × V) - T timesteps, V variables per step"
      bottlenecks: "Expression evaluation"
      
    monte_carlo:
      complexity: "O(N × T × V) - N runs"
      bottlenecks:
        - "Distribution sampling"
        - "Correlation (Cholesky decomposition) - not implemented"
      
  memory_usage:
    ast: "Linear in source size"
    ir: "Linear in AST size (~1.5x)"
    runtime_state: "O(T × V) - all timesteps stored"
    
  scalability_limits:
    model_size: "No hard limit, tested up to ~5KB source"
    time_horizon: "Practical limit ~1000 timesteps"
    monte_carlo_runs: "Practical limit ~100K runs"

# ============================================================================
# DEPENDENCY GRAPH
# ============================================================================

dependency_graph:
  
  external_dependencies:
    runtime:
      declared: []
      actual_usage:
        - "random (Python stdlib) - PRNG"
        - "json (Python stdlib) - IR serialization"
        - "hashlib (Python stdlib) - SHA-256 hashing"
      planned:
        - "numpy>=1.24.0 - Monte Carlo sampling, Cholesky"
        - "scipy>=1.10.0 - Distributions, statistical tests"
        
    development:
      declared:
        - "pytest>=7.0.0"
        - "pytest-cov>=4.0.0"
        - "mypy>=1.0.0"
        - "black>=23.0.0"
        - "ruff>=0.1.0"
        - "pre-commit>=3.0.0"
      usage: "None yet (no tests/CI)"
      
  internal_dependencies:
    compiler:
      lexer: []
      parser: ["lexer", "ast_nodes", "errors"]
      typechecker: ["ast_nodes", "errors"]
      provenance_checker: ["ast_nodes", "errors"]
      ir_generator: ["ast_nodes", "errors"]
      compiler: ["lexer", "parser", "typechecker", "provenance_checker", "ir_generator"]
      
    runtime:
      runtime: ["random", "json"]
      
    stdlib:
      all_modules: ["PEL language (imported at runtime)"]

# ============================================================================
# CI/CD CONFIGURATION
# ============================================================================

cicd_configuration:
  
  current_state: "NONE"
  
  version_control:
    system: "Git"
    hosting: "GitHub"
    main_branch: "main (assumed)"
    
  workflows:
    existing: []
    needed:
      - "test.yml - Lint, typecheck, test, coverage"
      - "conformance.yml - Run conformance suite"
      - "release.yml - Build and publish to PyPI"
      
  quality_gates:
    current: "None"
    needed:
      - "All tests pass"
      - "Coverage ≥80%"
      - "Type checking clean (mypy)"
      - "Linting clean (ruff, black)"
      
  release_process:
    current: "Manual (not yet released)"
    versioning: "Semantic versioning (0.1.0)"
    changelog: "None yet"
    distribution: "PyPI (planned: pel-lang package)"

# ============================================================================
# TESTING COVERAGE
# ============================================================================

testing_coverage:
  
  current_state:
    unit_tests: 0
    integration_tests: 0
    conformance_tests: 0
    benchmarks: 0
    total_coverage: "0%"
    
  test_infrastructure:
    framework: "pytest (declared, not used)"
    coverage_tool: "pytest-cov (declared, not used)"
    fixtures: "None"
    test_data: "examples/saas_subscription.pel (could be test case)"
    
  testing_gaps:
    critical:
      - "No unit tests for lexer"
      - "No unit tests for parser"
      - "No unit tests for type checker"
      - "No unit tests for runtime"
      - "No determinism tests (same seed → same results)"
      - "No regression tests"
      
    important:
      - "No integration tests (end-to-end compilation + execution)"
      - "No conformance tests (spec compliance)"
      - "No property-based tests (fuzzing)"
      - "No benchmark implementations"
      
    nice_to_have:
      - "No performance regression tests"
      - "No security tests (sandbox escape attempts)"
      - "No stress tests (large models, long time horizons)"

# ============================================================================
# KNOWN ISSUES AND GAPS
# ============================================================================

known_issues:
  
  critical:
    - issue: "Duration literal tokenization incomplete"
      location: "compiler/lexer.py:195-210"
      impact: "Breaks rate expressions like $500/1mo"
      status: "known, documented in IMMEDIATE_FIXES_NEEDED.md"
      
    - issue: "Per-duration expression parsing incomplete"
      location: "compiler/parser.py"
      impact: "Rate per TimeUnit types not fully supported"
      status: "known, documented"
      
    - issue: "Distribution named arguments may not parse"
      location: "compiler/parser.py"
      impact: "~Normal(μ=0.12, σ=0.03) syntax uncertain"
      status: "needs verification"
      
  major:
    - issue: "Zero test coverage"
      impact: "No regression prevention, no correctness verification"
      status: "blocking production use"
      
    - issue: "8 of 9 stdlib modules incomplete"
      impact: "Severely limited functionality"
      status: "ongoing work needed"
      
    - issue: "No CI/CD"
      impact: "No automated quality checks"
      status: "infrastructure gap"
      
  minor:
    - issue: "Correlation sampling not implemented"
      location: "runtime/runtime.py"
      impact: "Monte Carlo can't handle correlated variables"
      status: "advanced feature"
      
    - issue: "Sensitivity analysis incomplete"
      location: "runtime/runtime.py"
      impact: "Tornado charts available, Sobol indices not"
      status: "advanced feature"

# ============================================================================
# REPOSITORY METADATA
# ============================================================================

repository_metadata:
  license: "Dual (AGPL-3.0-or-later OR Commercial)"
  cla_required: true
  python_version: ">=3.10"
  project_status: "Pre-production (0.1.0)"
  maturity: "Alpha"
  
  documentation_completeness:
    specifications: "100% (11 documents)"
    api_docs: "0%"
    user_guide: "30%"
    tutorials: "20%"
    
  community:
    contributors: "1 (assumed)"
    issues: "Tracked in IMMEDIATE_FIXES_NEEDED.md"
    pull_requests: "None yet"

# ============================================================================
# END OF SNAPSHOT
# ============================================================================
